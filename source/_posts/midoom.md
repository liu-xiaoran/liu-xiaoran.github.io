---
title: 线上OOM分析 2/119
date: 2021-11-19 19:36:45
---

## 记一次mybatis引起的线上OOM

最近在忙着加班+找房换租的窝，所有很久没写博客，社畜的艰苦生活，哈哈

言归正传，最近线上一个小项目，出现了特殊的OOM情况，很有意思，所以跟大家分享一下

### 奇特的内存溢出现象
#### 项目背景：
公司有某个线下零售业务部门，有POS一千台左右，业务和门店还在持续扩张中。业务现有的CRM和POS修改起来较为困难(全为供应商标准系统)。
业务有个中间件，用于接受POS的一个结账前的会员信息请求，转发到SRM系统获取基础信息后，然后查数据库表做数据加工后再返回POS，以此来做双方系统的自定义配置。(个人不认可这种方案)
这个小项目是由我们组的一个小伙伴完成开发，压测完成后部署在阿里云上，2台ECS做了负载均衡。

#### 溢出现场:
项目上线后，间隔一段时间后，用户反馈POS有些门店请求报错。上服务器查询日志发现报了内存溢出，项目已经被守护进程自动重启，无法查看溢出时内存具体成分。查看代码报错，是请求导致的OOM报错。结合之前的日志分析发现，崩溃前已经发生多次内存无法回收的异常。查看ECS的监控，发现内存是缓慢上升的。断定发生了内存泄漏。
监控了一段时间后，发现被重启后应用，内存依旧在缓慢上升。并发高发时间段是在晚上7-8点，而内存溢出发生在晚上9点多。所以也并不是并发引起的。思考这种内存无法释放的缓慢OOM，单靠增加内存或者JVM调优是很难解决的，应该是小伙伴的代码中有对象没有及时释放导致的，此时已经晚上十点多，门店已经关店，无法观察内存变化，而且之前压测也没有出现OOM。考虑有可能需要长压测来让内存缓慢累计，所有起了压测程序，参数并发调低，压测时间改为连续3小时。等待明早观察成果。

### 问题定位
早上发现内存回收正常，尝试分析内存内容



### 问题解决
